import streamlit as st
from moviepy.editor import VideoFileClip, AudioFileClip
import tempfile
import whisper
from deep_translator import GoogleTranslator
import os
from pydub import AudioSegment
import asyncio
import edge_tts

st.set_page_config(page_title="Video Translation", layout="centered")
st.title("ğŸ¬ Automatic Video Translation")

voice_options = {
    "Ø£Ù†Ø«Ù‰ Ù…ØµØ±ÙŠØ©": {"ar": "ar-EG-SalmaNeural", "en": "en-US-JennyNeural"},
    "Ø°ÙƒØ± Ø¬Ø²Ø§Ø¦Ø±ÙŠ": {"ar": "ar-DZ-IsmaelNeural", "en": "en-US-EricNeural"},
    "Ø£Ù†Ø«Ù‰ Ø³Ø¹ÙˆØ¯ÙŠØ©": {"ar": "ar-SA-ZariyahNeural", "en": "en-US-EmmaMultilingualNeural"},
    "Ø°ÙƒØ± Ø³Ø¹ÙˆØ¯ÙŠ": {"ar": "ar-SA-HamedNeural", "en": "en-US-EricNeural"},
    "Ø£Ù†Ø«Ù‰ Ù„Ø¨Ù†Ø§Ù†ÙŠÙ‡": {"ar": "ar-LB-LaylaNeural", "en": "en-US-EmmaMultilingualNeural"},
}
voice_gender = st.selectbox("Select the type of voice", list(voice_options.keys()), index=0)

uploaded_video = st.file_uploader("Upload a video", type=["mp4", "mov", "avi", "mkv"])

async def convert_text_to_speech(text, lang, voice_gender):
    voice = voice_options[voice_gender][lang]
    output_path = os.path.join(tempfile.gettempdir(), "voice.mp3")
    communicate = edge_tts.Communicate(text=text, voice=voice)
    await communicate.save(output_path)
    return output_path

if uploaded_video:
    st.video(uploaded_video)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(uploaded_video.read())
        video_path = tmp.name

    try:
        #st.info("ğŸ§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ...")
        video_clip = VideoFileClip(video_path)
        audio_path = video_path.replace(".mp4", ".mp3")
        video_clip.audio.write_audiofile(audio_path)

        #st.info("ğŸ§  ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ (Whisper)...")
        model = whisper.load_model("base")
        result = model.transcribe(audio_path)
        original_text = result["text"]

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø£ØµÙ„ÙŠØ©
        source_lang = "ar" if result["language"] == "ar" else "en"
        target_lang = "en" if source_lang == "ar" else "ar"

        #st.subheader("ğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬:")
        #st.write(original_text)

        #st.info(f"ğŸŒ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ {'Ø¹Ø±Ø¨ÙŠØ©' if target_lang == 'ar' else 'Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©'}...")
        translated_text = GoogleTranslator(source='auto', target=target_lang).translate(original_text)

        #st.subheader("ğŸŒ Ø§Ù„ØªØ±Ø¬Ù…Ø©:")
        #st.write(translated_text)

        #st.info("ğŸ”Š ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¥Ù„Ù‰ ØµÙˆØª...")
        tts_path = asyncio.run(convert_text_to_speech(translated_text, target_lang, voice_gender))

        audio = AudioSegment.from_file(tts_path)
        original_audio_duration = audio.duration_seconds
        video_duration = video_clip.duration
        from pydub.effects import speedup

        # 3ï¸âƒ£ Ø¶Ø¨Ø· Ø§Ù„Ø³Ø±Ø¹Ø©
        if not video_duration or video_duration == 0:
            st.error("âŒ Ù…Ø¯Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ØºÙŠØ± ØµØ§Ù„Ø­Ø©.")
            st.stop()

        speed_factor = original_audio_duration / video_duration

        if speed_factor == 0 or speed_factor == float('inf'):
            st.warning("âš ï¸ Ø­ØµÙ„Øª Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø¶Ø¨Ø· Ø§Ù„Ø³Ø±Ø¹Ø©. Ù‡ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµÙˆØª ÙƒÙ…Ø§ Ù‡Ùˆ.")
            new_audio = audio
        else:
            try:
                new_audio = speedup(audio, playback_speed=speed_factor, crossfade=0)
            except ZeroDivisionError:
                st.warning("âš ï¸ Ø§Ù„ØµÙˆØª Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§ Ù„Ù„ØªØ¹Ø¯ÙŠÙ„ØŒ Ù‡ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ø¨Ø¯ÙˆÙ† ØªØ³Ø±ÙŠØ¹.")
                new_audio = audio

        adjusted_audio_path = os.path.join(tempfile.gettempdir(), "adjusted_voice.wav")
        new_audio.export(adjusted_audio_path, format="wav")

        #st.info("ğŸï¸ ØªØ±ÙƒÙŠØ¨ Ø§Ù„ØµÙˆØª Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¹Ù„Ù‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ...")
        final_path = os.path.join(tempfile.gettempdir(), "final_video.mp4")
        new_audio = AudioFileClip(adjusted_audio_path)
        new_video = video_clip.set_audio(new_audio)
        new_video.write_videofile(final_path, codec="libx264", audio_codec="aac")

        st.success("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡! Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ±Ø¬Ù…Ø©:")
        st.video(final_path)

    finally:
        try:
            video_clip.reader.close()
            if video_clip.audio:
                video_clip.audio.reader.close_proc()
        except:
            pass
        for f in [video_path, audio_path, tts_path]:
            if os.path.exists(f):
                try:
                    os.remove(f)
                except:
                    pass
